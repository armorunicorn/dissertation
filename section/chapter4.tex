\chapter{Attention-Based字词结合卷积循环中文短文本分类模型}
\label{4_section}
短文本分类在自然语言处理领域中扮演着重要的角色，
在垃圾信息过滤、语义分析、自动问答等任务中有着广泛的应用。
本节将通过结合基于局部感知域理论设计的卷积神经网络以及基于时间记忆理论设计的循环神经网络，
组合两种网络的优点，提出一种新型的中文短文本分类模型。该模型还引入了Attention机制等
现在最新的深度神经网络技术，并结合字向量与词向量
，优化神经网络提取出的文本特征向量，防止特征向量信息冗余或信息缺失，
增强模型在中文短文本上的分类效果。
\section{卷积循环神经特征提取网络}
\label{CLSTM_section}
根据\ref{cnn_section}小节与\ref{rnn_section}小节的介绍，我们知道
卷积神经网络和循环神经网络在特征提取方面都有优秀的表现，各有其优缺点。一方面，卷积神经网络
能够从序列数据（如文本数据）和空间数据（如图片数据）之中快速的学习本地特征，得到分类结果，
但忽略了特征的顺序，对某些数据并不适合；另一方面，循环神经网络则专门对序列数据
进行建模，但却无法并发的提取数据特征，使得模型运算较慢，无法大规模应用\citing{schmidhuber2015deep}。
针对这样的特点，本文将组合卷积神经网络与循环神经网络，把它们堆叠在一个系统中共同进行特征提取工作，
形成一个新的特征提取网络，有效的吸收两种模型的优点，同时避免其缺点。
\subsection{卷积循环神经网络整体结构}
卷积循环神经网络由卷积网络层和循环网络层两个主要模块组成，总体结构如图\ref{CLSTM}所示。
文本数据编码后，卷积网络层会对其进行处理，得到文本的初级特征图，特征图之后输入循环
网络层，进行更高一级的特征提取，并将网络中最后一个隐藏节点的输出向量作为
最终的特征向量。
\begin{figure}[h]
    \includegraphics[scale=0.4]{picture/CLSTM.png}
    \caption{卷积循环神经网络结构图}
    \label{CLSTM}
\end{figure}
\subsection{卷积网络层}
卷积网络层的结构与普通卷积神经网络大致相同，分为卷积模块、池化模块与光栅模块。
为了适应本文研究的中文短文本数据，本文对每个模块的具体实现做出了一些调整，下面
对各个模块进行说明：

（1）卷积模块

基于文本数据的信息特征，卷积网络层的卷积模块使用窄卷积（Narrow Convolution）
作为卷积策略，避免宽卷积（Wide Convolution）的补零操作对提取文本特征造成影响。
卷积核长度分为3、4、5三种，宽度为词向量的长度，滑动步长为1，保证特征图能够包含尽可能多的文本特征。
每种卷积核生成128个，保障特征图的多样性。

（2）池化模块

由于短文本数据的长度以及卷积核的长度并不统一，卷积层产生的特征图的长度各有不同，
为了让接下来的循环网络层能够处理特征图，需要在池化模块对其进行处理，将长度统一。
本文使用k-max池化（k-max pooling）作为
池化算法，该算法是Kalchbrenner提出的动态k-max算法的前置算法\citing{kalchbrenner2014convolutional}，
主要思想是对于一个给定的$k$值与特征序列$p$（$p\in R^p,length(p)\geq k$），选择序列$p$中前$k$个最大值，
且保留原来序列的次序（即生成序列是原序列$p$的一个子序列）。通过k-max池化算法，卷积网络层不仅能够
接收变长的输入，同时生成的特征图也保留了相对的位置信息，提升了特征的质量。

（3）光栅模块

光栅模块主要用来整合生成的各个特征图，将其中相同位置的特征值进行合并，形成最终的特征向量，输入下面的循环网络层，
如网络结构图\ref{CLSTM}中的虚线所示。

\subsection{循环网络层}

循环神经网络依据时间对序列数据进行建模，上一个节点的数据不仅会进行输出，还会作为同一隐藏层下一个节点的输入，
隐藏层最后一个节点则能够接收到全部的文本特征数据。但是短文本分类任务需要更多的考虑文本上下文信息，单纯使用正向
循环神经网络，模型只能够依据上文信息进行分类，无法利用下文的信息，从而影响最终的分类效果。因此，本文使用
双向长短时记忆循环神经网络（Bi-directional Long Short-Term Memory，Bi-LSTM）作为循环网络层的实现，
将网络分为前向传递层与后向传递层，分别获取输入短文本的上文信息与下文信息。

Bi-LSTM网络的整体流程如图\ref{Bi-LSTM}所示，对于从上层网络传来的输入向量，网络首先将其转变为
正序序列及逆序序列两个向量，然后分别输入一个单向LSTM网络进行特征提取，得到正序特征向量与逆序特征向量，
之后将两个特征向量合并，形成最终的文本特征向量并输出到下一层网络。经过这样的处理，网络提取的特征向量既包含
上文信息又包含下文信息，能够给之后的分类网络提供更加丰富的语义信息，有效缓解了短文本数据语义信息不足的问题
，提升了分类的准确率。
\begin{figure}[h]
    \includegraphics[scale=0.4]{picture/Bi-LSTM.png}
    \caption{Bi-LSTM网络流程图}
    \label{Bi-LSTM}
\end{figure}

\section{Attention Model}
Attention Model（注意力机制）是近几年兴起的一个新型模型，在很多场景被证明有效。
模型以认知心理学中的人脑注意力理论为核心，认为人脑在处理具体任务时，对于相关事物的注意力会集中在
某一个特定的地方，忽略其他无关的地方，而不是对每个地点分配相同的注意力。
通过这样的理论，Attention Model
能够合理的分配模型的计算资源，并且还可以避免非关键数据对结果的影响。
Attention Model最先在计算机视觉领域被
应用于图片识别等问题，取得了很好的成果，随后在自然语言处理领域也获得了优秀的成绩。
下面将以编码-解码模型中的Attention机制为例，
说明Attention Model的基本知识，然后在下一小节把Attention Model应用在卷积循环
神经网络当中。

编码-解码模型是自然语言处理领域中一个非常普遍的模型，它通过编码器将输入向量编码为中间
变量，再用解码器解码成输出向量，完成输入数据到另一组输出数据的转变，
从而实现多种内容转换任务，如机器翻译、文本复述等。
同时编码-解码模型的编码器与解码器没有具体限定，可以使用
各种各样的深度学习模型，如CNN、RNN等，让模型具有很强的泛用性，能够适合各种应用场景。
编码-解码模型的一般架构如图\ref{Encoder}所示。
\begin{figure}[h]
    \includegraphics[scale=0.5]{picture/Encoder.png}
    \caption{编码-解码模型架构图}
    \label{Encoder}
\end{figure}

编码-解码模型的输入一般是一个长度为$n$向量序列$X$（$X= \left \{x_1,x_2,x_3,...,x_n \right \}$）,
输出则是一个长度为$m$的向量序列$Y$（$Y= \left \{y_1,y_2,y_3,...,y_m \right \}$）。
模型运行时，编码器根据一定的映射规则对输入序列进行编码，得到中间编码$C$，表达公式如\ref{encode_eqn}所示。
\begin{equation}
    C=Encode\left ( x_1,x_2,x_3,...x_n \right )
    \label{encode_eqn}
\end{equation}
之后解码器对中间编码$C$进行解码，得到输出序列$Y$，计算公式如\ref{decode_eqn}所示。
\begin{equation}
    y_i=Decode\left ( C,y_1,y_2,y_3,...,y_{i-1} \right ),i=1,2,3,...,m
    \label{decode_eqn}
\end{equation}

可以看出解码器在计算输出序列$Y$时，对于每一个输出子项$y_i$用到的数据信息是一样的，都是
输入序列$X$编码后得到的中间编码$C$，
即输入序列$X$对输出序列$Y$中每一个子项的影响是相同的。这样的方式无疑忽略了
很多细节，在一些任务中甚至直接影响了输出结果。比如机器翻译任务，
一个词的翻译虽然与上下文和词本身同时有关，但对于人名机构名等名词，
翻译时词本身的影响必定较重，而其他的动词形容词等则可能上下文的影响较大，所以简单
输入序列同比例的作用在输出序列之上并不是一个好的方法。

Attention Model的出现很好的解决了这个问题，在Attention-Based编码-解码模型中，
编码器的输出不再是一个完整的中间编码$C$，而是输入序列$X$在编码阶段的历史状态$C_i$，
如公式\ref{attention_encode_eqn}所示。
\begin{equation}
    C_i=\sum_{j=1}^{n}\alpha_{ij}Encode\left ( x_j \right )
    \label{attention_encode_eqn}
\end{equation}
其中$\alpha_{ij}$表示当前输入子项$x_j$的注意力权重，这个值越高，表明$x_j$对结果
越重要。

解码器根据历史状态$C_i$得到优化后的输出序列，如公式\ref{attention_decode_eqn}所示。
\begin{equation}
    y_i=Decode\left ( C_i,y_1,y_2,y_3,...y_{i-1} \right ),i=1,2,3,...,m
    \label{attention_decode_eqn}
\end{equation}

可以发现，Attention Model本质上既是在模型的输出结果上添加一个注意力权重，
筛选对结果重要的数据项，同时过滤对结果影响较低的数据项，以此优化模型的输出效果。
这一思想在短文本分类任务中同样适用，因为对于短文本文本分类任务，确定分类结果的
往往是文本的几个关键词，如情感分析中的“喜欢”、“讨厌”等词，如果能够判断出这些关键词，
就可以极大的增强分类模型的结果。

\section{Attention-Based字词结合卷积循环中文短文本分类网络}
\subsection{Attention-Based卷积循环特征提取网络}
\label{ACLSTM_section}
根据Attention Model的思想，
本文改进了\ref{CLSTM_section}小节提出的卷积循环特征提取网络，
增加注意力权重，优化原始网络产生的特征向量。

Attention Model的核心在于注意力权重的获取，有多种实现方式，如Soft Attention、
Hard Attention等\citing{xu2015show}。为了减少模型的计算负担，
本文选择参数化的Soft Attention实现方式，该实现方式能够让整个Attention层直接嵌入
模型，使梯度可以经过Attention Mechanism模块，反向传播到其他地方，以此简化模型的训练过程。
总体计算流程如图\ref{Soft_Attention}所示，
首先把双层LSTM的输出$h$（$h=\left \{ h_1,h_2,..,h_n\right\}$）
送入一个单层全连接神经网络，根据\ref{attention_eqn1}公式将其转换为中间向量$u$，
作为原始输出$h$的隐藏表示。
\begin{equation}
    u_i=\tanh\left ( W_wh_i+b_w \right )
    \label{attention_eqn1}
\end{equation}

然后通过softmax函数计算中间向量$u$与文本上下文向量$u_w$的相似度$\alpha$，如公式\ref{attention_eqn2}所示。
\begin{equation}
    \alpha_i=\frac{\exp\left ( u_{i}^{\top }u_w \right )}{\sum_n\exp\left ( u_{i}^{\top }u_w \right )}
    \label{attention_eqn2}
\end{equation}

其中$u_w$是记忆向量，可以视为筛选重要特征的高层抽象参数，代表着
整个语料库的上下文信息\citing{sukhbaatar2015end}，在模型初始阶段随机初始化，
并在训练时同其他参数一起调整。计算结果$\alpha$则是标准化的注意力权重，用于下一步计算。

最后，把原始向量$h$和$\alpha$加权相加，就得到Attention优化后的特征向量$S$，如公式\ref{attention_eqn3}所示。
\begin{equation}
    S_i=\sum_n\alpha_ih_i
    \label{attention_eqn3}
\end{equation}
\begin{figure}[h]
    \includegraphics[scale=0.5]{picture/Attention.png}
    \caption{Soft Attention计算流程}
    \label{Soft_Attention}
\end{figure}

\subsection{双通道字词结合短文本分类模型}
在短文本分类任务中，词向量与字向量都能够作为文本的向量表示，但都有其固有的缺点：
词向量极度依赖分词结果，错误的分词会极大的影响词向量的性能；字向量虽然不存在分词的问题，但有些词语和
内部的汉字意思可能南辕北辙，如“东西”与其内部的汉字“东”和“西”，单纯从字向量很难推测出词的含义。
为了解决这个问题，本文设计了双通道字词结合短文本分类模型，同时接受短文本的词向量表示与字向量表示，
充分结合两者的特征信息，克服了常规分类模型的不足。

\begin{figure}[h]
    \includegraphics[scale=0.4]{picture/classifier.png}
    \caption{双通道字词结合短文本分类模型}
    \label{classifier}
\end{figure}

双通道字词结合短文本分类模型主要是构建一个包含两个平行的特征提取网络的分类模型，分别提取词向量特征与字向量特征，
总体结构如图\ref{classifier}所示。

模型总共分为三层：编码层、特征提取层与全连接层。编码层根据相应的词向量与字向量模型，将输入文本解析为词向量序列$W$
和字向量序列$C$。特征提取层分为两个平行的神经网络模块，由前文中的Attention-Based卷积循环特征提取网络组成，
分别提取词向量序列和字向量序列的文本特征，即向量$S_w$、$S_c$，然后根据公式\ref{classifier_eqn}将其合并，得到最终的文本特征向量$S$。
\begin{equation}
    S =\left [ S_w\bigoplus S_c \right ]
    \label{classifier_eqn}
\end{equation}

全连接层由线性转换层和Softmax层组成，其中线性转换层将特征向量$S$转换为一个维度与分类类别相当的实值向量，然后
Softmax层将这些实值映射为最终的条件概率，计算公式如\ref{softmax_eqn}所示。
\begin{equation}
    P=softmax\left ( W_sS+b_s \right )
    \label{softmax_eqn}
\end{equation}

本文使用公式\ref{cost_function}作为模型的损失函数，来最小化模型的分类误差。其中$N_t$表示训练语料库大小，
$N_c$表示类别数量，$P_{j}^{g}\left ( s_i \right )$表示当前文本的真实类别为$j$的概率，属于类别$j$则为1，否则为0。
%并且使用随机梯度下降算法（Stochastic Gradient Descent，SGD）作为训练的优化算法。
\begin{equation}
    Loss=-\sum_{i=1}^{N_t}\sum_{j=1}^{N_c}P_{j}^{g}\left ( s_i \right )\cdot\log\left ( P_j\left ( s_i \right ) \right )
    \label{cost_function}
\end{equation}
\section{实验设计和结果分析}
为了验证Attention-Based字词结合卷积循环中文短文本分类网络模型的可行性及有效性，本文
设计并实现了字向量/词向量两类总计8组对比实验模型，对新闻标题数据进行分类，并对结果进行分析。
\subsection{实验语料数据}
本实验所用的短文本数据由两部分组成，一部分来自“今日头条”网站，
另一部分为清华大学THUCTC新闻语料\citing{li2007scalable}。
语料库总计包含830396条新闻标题数据，共分为财经、科技、体育等12个类别（具体信息如表\ref{train_data_table}所示），
长度都在60字以内。
实验随机抽取80\%数据作为模型训练数据集，另外20\%则为验证集。
\begin{table}[h]
    \caption{实验语料库分类详情}
    \begin{tabular}{|c|c|c|c|}
        \hline
        类别 & 数量 & 类别 & 数量 \\
        \hline
        社会 & 57860 & 体育 & 134680 \\
        \hline
        时政 & 62658 & 股票 & 149853 \\
        \hline
        教育 & 41827 & 娱乐 & 92886 \\
        \hline
        财经 & 41085 & 家居 & 32188 \\
        \hline
        游戏 & 24313 & 房产 & 18440 \\
        \hline
        时尚 & 13150 & 科技 & 161456 \\
        \hline
    \end{tabular}
    \label{train_data_table}
    \end{table}

\subsection{实验环境及相关工具}
本实验模型采用机器学习库Tensorflow作为分类模型的实现工具。Tensorflow是
Google公司与2015年11月9日发布并宣布开源的机器学习框架，是现在最流行的深度学习项目之一。
Tensorflow支持Linux平台、Windows平台、Mac平台等多种主流平台，提供了非常丰富的
深度学习相关API，包括基本的向量矩阵计算、各种优化算法、
各种卷积神经网络和循环神经网络基本单元的实现、
以及可视化的辅助工具等等。
Tensorflow将所有用户输入的计算结构都视为数据流图（data flow graphs），图中的
节点（nodes）表示数学操作，连接节点的线表示在节点间相互输送数据的多维数据数组，
即张量（tensor）。
Tensorflow可以适应任何硬件环境，无论台式机、服务器、手机移动设备都可以
直接运行计算代码，并且拥有自动求微分功能，在模型训练时自动计算相关的微分导数，极大的简化了
深度学习代码的开发。

本实验使用英伟达GeForce显卡作为模型训练的辅助工具。
通过英伟达推出的CUDA技术（Compute Unified Device Architecture，统一计算架构），
使用GeForce显卡能够在在GPUs（GPGPU）上使用图形APIs进行传统通用计算，从而加速模型训练。

本实验具体的实验环境如表\ref{train_env_table}所示：
\begin{table}[h]
    \caption{实验环境具体配置}
    \begin{tabular}{|c|c|}
        \hline
        操作系统 & Ubuntu 16.04 \\
        \hline
        开发语言 & Python 3.6 \\
        \hline
        开发平台 & Google Tensorflow深度学习框架 \\
        \hline
        CPU & Intel I5 \\
        \hline
        内存 & 8G \\
        \hline
        硬盘 & 1T \\
        \hline
        显卡 & NVIDIA GeForce 1080 \\
        \hline
    \end{tabular}
    \label{train_env_table}
\end{table}
\subsection{分类模型评价方法}
分类模型的有效性有多种评价方法，如准确率（Precision）、
召回率（Recall）和F1-Measure。对于分类模型的分类结果，总体可以分为4中情况，如表\ref{classification_demo_table}所示。
\begin{table}[h]
    \caption{分类结果示例表}
    \begin{tabular}{|c|c|c|}
        \hline
        & 实际属于该类 & 实际不属于该类 \\
        \hline
        被分到该类 & TP & FP \\
        \hline
        未被分到该类 & FN & TN \\
        \hline
    \end{tabular}
    \label{classification_demo_table}
\end{table}
其中TP表示实际属于该类且被模型分到该类的样例数。FP表示实际不属于该类，但被
模型分到该类的样例数。FN表示实际属于该类，但未被分到该类的样例数。TN表示实际不属于该类，
且未被分到该类的样例数。

根据表\ref{classification_demo_table}，准确率计算公式可写为式\ref{accuracy}。
\begin{equation}
    Accuracy=\frac{TP+TN}{TP+FP}
    \label{accuracy}
\end{equation}
召回率可定义为式\ref{recall}。
\begin{equation}
    Recall=\frac{TP+TN}{TP+FN}
    \label{recall}
\end{equation}
准确率反映了模型分类结果的正确率，召回率反映了模型找到正确列表的能力，两者都可以
体现分类模型有效性，但很多时候会相互矛盾。为了正确评价本实验中实现的所有模型，本文使用
这两个的加权调和平均值作为评价标准，即F1-Measure，计算公式如\ref{f1-measure}所示。
\begin{equation}
    F1-Measure=\frac{2\times Precision \times Recall}{Precision+Recall}
    \label{f1-measure}
\end{equation}
\subsection{对比实验设计}
为了验证本文设计的模型对于短文本分类的效果，本实验设计了4组对比实验，每组又词向量分类模型与
字向量分类模型两种，总计8个对比实验：

（1）传统LSTM模型

模型结构与实现方式由文献\cite{zhou2016compositional}提供，
词向量版本使用Ansj中文分词开源库以及word2vec工具将训练语料映射为200维的词向量文本数据，
字向量版本使用word2vec工具将训练语料映射为200维的字向量文本数据。隐藏层的节点个数设置为200。
模型训练的学习率（learn rate）取0.001，优化算法使用随机梯度下降算法（Stochastic Gradient Descent，SGD）。
分类器使用多类分类的逻辑回归分类器，使用LSTM模型隐藏层最后一个节点的输出值作为分类特征向量。训练循环次数为50，
批尺寸（batch size）为200。

（2）传统CNN模型

模型结构与实现方式同样由文献\cite{zhou2016compositional}提供，
词向量版本使用Ansj中文分词开源库以及word2vec工具将训练语料映射为200维的词向量文本数据，
字向量版本使用word2vec工具将训练语料映射为200维的字向量文本数据。卷积层使用大小为3、4、5的三类卷积核，每类卷积核的个数为128。
池化层使用最大池化算法。
模型训练的学习率（learn rate）取0.001，优化算法使用随机梯度下降算法（Stochastic Gradient Descent，SGD）。
分类器使用多类分类的逻辑回归分类器。训练循环次数为50，
批尺寸（batch size）为200。

（3）传统卷积循环网络模型（CLSTM）

模型依据文献\cite{Zhou2015A}实现，
词向量版本使用Ansj中文分词开源库以及word2vec工具将训练语料映射为200维的词向量文本数据，
字向量版本使用word2vec工具将训练语料映射为200维的字向量文本数据。
卷积层使用大小为3、4、5的三类卷积核，每类卷积核的个数为128。
池化层使用最大池化算法。
LSTM层的隐藏节点个数设置为200。
模型训练的学习率（learn rate）取0.001，优化算法使用随机梯度下降算法（Stochastic Gradient Descent，SGD）。
分类器使用多类分类的逻辑回归分类器。训练循环次数为50，
批尺寸（batch size）为200。

（4）Attention-Based卷积循环网络模型(Attention-CLSTM)

模型依据\ref{ACLSTM_section}小节的设计实现，
词向量版本使用Ansj中文分词开源库以及word2vec工具将训练语料映射为200维的词向量文本数据，
字向量版本使用word2vec工具将训练语料映射为200维的字向量文本数据。
卷积层使用大小为3、4、5的三类卷积核，每类卷积核的个数为128。
池化层使用k-max池化算法。
LSTM层的隐藏节点个数设置为200。
Attention层隐藏参数向量长度为200。
模型训练的学习率（learn rate）取0.001，优化算法使用随机梯度下降算法（Stochastic Gradient Descent，SGD）。
分类器使用多类分类的逻辑回归分类器。训练循环次数为50，
批尺寸（batch size）为200。

本文设计的分类模型同时使用词向量文本数据与字向量文本数据，都为200维。两个通道的
卷积层都使用大小为3、4、5的三类卷积核，每类卷积核的个数为128，池化层都使用k-max池化算法，
LSTM层的隐藏节点数目与Attention层的隐藏参数向量长度也都为200。
模型训练的学习率（learn rate）取0.0001，优化算法使用随机梯度下降算法（Stochastic Gradient Descent，SGD）。
分类器使用多类分类的逻辑回归分类器。训练循环次数为50，
批尺寸（batch size）为200。
\subsection{实验结果和分析}
本文对上一个小节设计的实验模型在新闻标题语料库上进行实验，每个模型都训练5次并选取平均结果。
实验对比结果如表\ref{classification_result_table}所示。
\begin{table}[h]
    \caption{实验对比结果}
    \begin{tabular}{|c|c|c|}
        \hline
        模型 & 字向量 & 词向量 \\
        \hline
        传统CNN & 0.855679 & 0.835943 \\
        \hline
        传统LSTM & 0.869336 & 0.849313 \\
        \hline
        CLSTM & 0.894824 & 0.872391 \\
        \hline
        Attention-Based CLSTM & 0.89947 & 0.87346 \\
        \hline
        双通道Attention-Based CLSTM（本文设计） & \multicolumn{2}{|c|}{0.913441} \\
        \hline
    \end{tabular}
    \label{classification_result_table}
\end{table}
从实验结果的1到2行中可以看出，和字向量相比，词向量在短文本分类任务中效果更好，这表明
词向量在文本特征表达上更加有效，并且使用LSTM结构的循环神经网络比卷积神经网络更加适合
提取文本特征。表格的第3行与第四行证明结合CNN与RNN网络能够有效提升模型对文本数据的表达能力。
最后，对比其余模型，本文设计的双通道Attention-Based CLSTM分类模型取得了最好的结果，证明了
字向量文本特征能够很好的强化原本的词向量文本特征，从而让模型能够进一步的获取短文本的语义，
有效提高短文本分类的准确率。
\section{本章小结}
 本章主要提出了Attention-Based字词结合卷积循环中文短文本分类模型。
 首先介绍了用于提取文本特征的卷积循环神经网络，然后再分析了Attention Model的相关
 概念并引入卷积循环神经网络。之后加入字向量数据，设计了一个双通道的短文本分类模型，
 弥补了使用单一词向量/字向量的不足。
 最后通过对比实验，验证了模型的可行性及有效性。